{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25f69f0",
   "metadata": {},
   "source": [
    "# Fake News Detection\n",
    "### crawling data and constructing database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import json\n",
    "import pyrebase\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab08cb6",
   "metadata": {},
   "source": [
    "Outline:          \n",
    "From news website homepage, crawl all news address. Next, for each news address, extract 'title', 'author', 'text' and 'url' 4 features, then send it to cloud database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d282a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getJson(siteUrl):\n",
    "\n",
    "    # crawl json data from 'https://news.yahoo.com/'\n",
    "\n",
    "    # the news platform homepage to crawl\n",
    "    siteR = requests.get(siteUrl)\n",
    "    \n",
    "    # check if response succeed\n",
    "    #print(siteUrl, siteR.ok) #True\n",
    "\n",
    "    siteSel = etree.HTML(siteR.content)\n",
    "    \n",
    "    return siteSel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2771ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsList_yahoo(siteSel):\n",
    "\n",
    "    # get the news web address list from homepage of news platform\n",
    "    newsList = []\n",
    "\n",
    "    for quote in siteSel.xpath('//div[@id=\"Main\"]//@href'):\n",
    "        \n",
    "        # exclude video news\n",
    "        if 'video' in quote: continue\n",
    "       \n",
    "        #f ix incomplete link address\n",
    "        if 'https' not in quote:\n",
    "            quote = 'https://news.yahoo.com/'+quote[1:]\n",
    "        newsList.append(quote)\n",
    "\n",
    "    #print('from yahoo,total',len(newsList),'urls')\n",
    "    \n",
    "    return newsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4762d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsList_fox(siteSel):\n",
    "\n",
    "    # get the news web address list from homepage of news platform\n",
    "    newsList = []\n",
    "\n",
    "    for quote in siteSel.xpath('//div[@class=\"content article-list\"]//h2[@class=\"title\"]//@href'):\n",
    "       \n",
    "        if 'video' in quote: continue\n",
    "    \n",
    "        if 'https' not in quote:\n",
    "            quote = 'https://news.foxnews.com/'+quote[1:]\n",
    "        newsList.append(quote)\n",
    "\n",
    "    #print('from fox,total',len(newsList),'urls')\n",
    "    \n",
    "    return newsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2935d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsList_huffpost(siteSel):\n",
    "\n",
    "    # get the news web address list from homepage of news platform\n",
    "    newsList = []\n",
    "\n",
    "    for quote in siteSel.xpath('//div[@class=\"inner-group\"]//div[@class=\"card__headlines\"]//@href'):\n",
    "    \n",
    "        if 'video' in quote:continue\n",
    "        \n",
    "        if 'https' not in quote:\n",
    "            quote = 'https://www.huffpost.com/'+quote[1:]\n",
    "        newsList.append(quote)\n",
    "\n",
    "    #print('from huffpost,total',len(newsList),'urls')\n",
    "    \n",
    "    return newsList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466324ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsList_buzzfeed(siteSel):\n",
    "\n",
    "    # get the news web address list from homepage of news platform\n",
    "    newsList = []\n",
    "\n",
    "    for quote in siteSel.xpath('//div[@class=\"news-feed grid-layout-main\"]//article//h2//@href'):\n",
    "    \n",
    "        if 'video' in quote:continue\n",
    "        \n",
    "        if 'https' not in quote:\n",
    "            quote = 'https://www.buzzfeednews.com/'+quote[1:]\n",
    "        newsList.append(quote)\n",
    "\n",
    "    #print('from buzzfeed,total',len(newsList),'urls')\n",
    "    \n",
    "    return newsList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412764cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures_yahoo(url):\n",
    "    \n",
    "    newsR = requests.get(url)\n",
    "    if newsR.ok is False: return\n",
    "        \n",
    "    newsSel = etree.HTML(newsR.text)\n",
    "\n",
    "    # extract features\n",
    "    [title] = newsSel.xpath('//head//title/text()')\n",
    "    \n",
    "    author_ = newsSel.xpath('//body//article[@role=\"article\"]//script[@type=\"application/ld+json\"][1]/text()')\n",
    "    author_ = json.loads(author_[0].strip())['author']['name']\n",
    "    author = [author_]\n",
    "\n",
    "    text=str()\n",
    "    for t in newsSel.xpath('//article//div[@class=\"caas-body\"]/p/text()'):\n",
    "        t += '\\n' # add linebreak if needed\n",
    "        text += t\n",
    "\n",
    "    newsFeatures = {'title':title,'author':author,'text':text,'url':url}\n",
    "    \n",
    "    # remove missing values \n",
    "    if '' in newsFeatures.values() or [] in newsFeatures.values():\n",
    "        return None\n",
    "    else:\n",
    "        return newsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877ce95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getFeatures_fox(url):\n",
    "    \n",
    "    newsR = requests.get(url)\n",
    "    if newsR.ok is False: return\n",
    "    \n",
    "    newsSel = etree.HTML(newsR.text)\n",
    "    \n",
    "    [title] = newsSel.xpath('//header//h1/text()')\n",
    "    author = newsSel.xpath('//div[@class=\"author-byline\"]//a/text()')\n",
    "    \n",
    "    text=str()\n",
    "    for t in newsSel.xpath('//article//div[@class=\"article-body\"]/p/text()'):\n",
    "        t += '\\n'\n",
    "        text += t\n",
    "    \n",
    "    newsFeatures = {'title':title,'author':author,'text':text,'url':url}\n",
    "    \n",
    "    if '' in newsFeatures.values() or [] in newsFeatures.values():\n",
    "        return None\n",
    "    else:\n",
    "        return newsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1cc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures_huffpost(url):\n",
    "    \n",
    "    newsR = requests.get(url)\n",
    "    if newsR.ok is False: return\n",
    "        \n",
    "    newsSel = etree.HTML(newsR.text)\n",
    "\n",
    "    \n",
    "    [title] = newsSel.xpath('//*[@id=\"entry-header\"]/header//h1/text()')\n",
    "\n",
    "    author = newsSel.xpath('//div[@class=\"entry__byline__author\"]/a[1]/@data-vars-item-name')\n",
    "\n",
    "    text=str()\n",
    "    for t in newsSel.xpath('//*[@id=\"entry-body\"]//div[@class=\"primary-cli cli cli-text \"]//p/text()'):\n",
    "        t += '\\n' # add linebreak if needed\n",
    "        text += t\n",
    "\n",
    "    newsFeatures = {'title':title,'author':author,'text':text,'url':url}\n",
    "    \n",
    "    if '' in newsFeatures.values() or [] in newsFeatures.values():\n",
    "        return None\n",
    "    else:\n",
    "        return newsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee478fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures_buzzfeed(url):\n",
    "    \n",
    "    newsR = requests.get(url)\n",
    "    if newsR.ok is False: return\n",
    "        \n",
    "    newsSel = etree.HTML(newsR.text)\n",
    "\n",
    "    \n",
    "    [title] = newsSel.xpath('//*[@id=\"buzz-content\"]//article//h1/text()')\n",
    "\n",
    "    author = newsSel.xpath('//article//span[@class=\"metadata-link headline-byline_bylineName__D9j7i\"]/text()')\n",
    "\n",
    "    text=str()\n",
    "    for t in newsSel.xpath('//div[@class=\"js-subbuzz-wrapper\"]//p/text()'):\n",
    "        t += '\\n' # add linebreak if needed\n",
    "        text += t\n",
    "\n",
    "    newsFeatures = {'title':title,'author':author,'text':text,'url':url}\n",
    "    \n",
    "    if '' in newsFeatures.values() or [] in newsFeatures.values():\n",
    "        return None\n",
    "    else:\n",
    "        return newsFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d1646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect2db():\n",
    "    # connect to cloud dastabase\n",
    "    # initialize cloud database 'fakenews'\n",
    "\n",
    "    config = {\"apiKey\": \"apiKey\",\n",
    "              \"authDomain\": \"projectId.firebaseapp.com\",\n",
    "              \"databaseURL\": \"https://fakenews-53e87-default-rtdb.firebaseio.com/\",\n",
    "              \"storageBucket\": \"projectId.appspot.com\",\n",
    "              \"serviceAccount\": \"fakenews-53e87-firebase-adminsdk-3wmt8-e668bd4ef8.json\"\n",
    "              }\n",
    "    firebase = pyrebase.initialize_app(config)\n",
    "    db = firebase.database()\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72142601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openRecords():\n",
    "    # create records of id-url pairs of news\n",
    "\n",
    "    f = open('newsURLs.csv','a+',encoding='utf-8')\n",
    "    writer = csv.writer(f)\n",
    "    newsURLs = pd.read_csv('newsURLs.csv')\n",
    "    print('number of news now:',len(newsURLs))\n",
    "\n",
    "    return newsURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    print('----------------')\n",
    "    print(datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S'))\n",
    "    \n",
    "    # connect to cloud dastabase and initialize cloud database\n",
    "    db = connect2db()\n",
    "    # open newsURLs.csv for recording\n",
    "    newsURLs = openRecords()\n",
    "    \n",
    "    # news websites we focus on\n",
    "    sites = ['https://www.yahoo.com/',\n",
    "             'https://www.foxnews.com/',\n",
    "             'https://www.huffpost.com/',\n",
    "             'https://www.buzzfeednews.com/']\n",
    "\n",
    "    for siteUrl in sites:\n",
    "\n",
    "        try:\n",
    "\n",
    "            siteSel = getJson(siteUrl)\n",
    "            # get news list\n",
    "            if siteUrl == 'https://www.yahoo.com/':\n",
    "                newsList = getNewsList_yahoo(siteSel)\n",
    "            elif siteUrl == 'https://www.foxnews.com/':\n",
    "                newsList = getNewsList_fox(siteSel)\n",
    "            elif siteUrl == 'https://www.huffpost.com/':\n",
    "                newsList = getNewsList_huffpost(siteSel)\n",
    "            elif siteUrl == 'https://www.buzzfeednews.com/':\n",
    "                newsList = getNewsList_buzzfeed(siteSel)\n",
    "        except Exception as e:\n",
    "            print(\"error:\", e)\n",
    "            continue\n",
    "\n",
    "        # put features of each news to cloud database\n",
    "        for url in newsList:\n",
    "\n",
    "            try:\n",
    "                # delete dubplicates from newsList and append it to newsURLs.csv\n",
    "                f = open('newsURLs.csv', 'a+', encoding='utf-8')\n",
    "                writer = csv.writer(f)\n",
    "\n",
    "                # remove url if duplicate with urls before\n",
    "                newsURLs = pd.read_csv('newsURLs.csv')\n",
    "                # print('now the records have total',len(newsURLs))\n",
    "\n",
    "                if url in newsURLs['url'].values:\n",
    "                    newsList.remove(url)\n",
    "                    continue\n",
    "\n",
    "                # get features of each news\n",
    "                # newsFeatures = {'title':title,'author':author,'text':text}\n",
    "                if siteUrl == 'https://news.yahoo.com/':\n",
    "                    newsFeatures = getFeatures_yahoo(url)\n",
    "                elif siteUrl == 'https://www.foxnews.com/':\n",
    "                    newsFeatures = getFeatures_fox(url)\n",
    "                elif siteUrl == 'https://www.huffpost.com/':\n",
    "                    newsFeatures = getFeatures_huffpost(url)\n",
    "                elif siteUrl == 'https://www.buzzfeednews.com/':\n",
    "                    newsFeatures = getFeatures_buzzfeed(url)\n",
    "\n",
    "\n",
    "                # save data to cloud database when newsFeatures is non-empty\n",
    "                if newsFeatures is not None:\n",
    "                    # write distinct id in newsURLs.csv for records\n",
    "                    i = len(newsURLs)\n",
    "                    writer.writerow([i, url])\n",
    "                    f.close\n",
    "\n",
    "                    db.child(\"news\").child(i).set(newsFeatures)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"error:\", e)\n",
    "                continue\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdb8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "2022/04/25 22:41:21\n",
      "number of news now: 337\n",
      "error: not enough values to unpack (expected 1, got 0)\n"
     ]
    }
   ],
   "source": [
    "# execute programe every 30 minutes, to enable data flow\n",
    "\n",
    "main()\n",
    "\n",
    "schedule.every(30).minutes.do(main)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9196ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
